{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "#import org.apache.spark.sql.functions.monotonicallyIncreasingId\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf, SparkContext ,SQLContext,Row,HiveContext\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "#import org.apache.spark.sql.expressions._\n",
    "from pyspark import HiveContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, functions as F\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import broadcast\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import hashlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#variables\n",
    "eff_close_dt = '3099-12-31'\n",
    "current_bus_dt='2018-02-18'\n",
    "eff_flag_curr = 'Y'\n",
    "eff_flag_non_curr = 'N'\n",
    "eff_start_date_hist = 'Today'\n",
    "eff_start_date_delta = 'Tomorrow' ## 1 day lead\n",
    "dt=datetime.now()\n",
    "load_tm=dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Day 0 Load\n",
    "day0_data=spark.read.csv(\"/Users/sandip/Data/spark/SCD2/store.csv\",header='true',inferSchema='true')\n",
    "#Surrogate key generation\n",
    "day0_data_with_sk=day0_data.select(\"Store_ID\").withColumn(\"store_id_sk\",F.row_number().over(Window.partitionBy(lit(1)).orderBy(lit(1))))\n",
    "#add SCD2 column\n",
    "day0_sk_df = day0_data.join(broadcast(day0_data_with_sk) ,'Store_ID','inner').withColumn('eff_start_date',lit(current_bus_dt)).withColumn('eff_end_date',lit(eff_close_dt)).withColumn('load_tm',lit(load_tm)).withColumn('current_flag',lit(eff_flag_curr)).withColumn('is_delta',lit('N'))\n",
    "#Save data in parquet format with partition on store id\n",
    "day0_sk_df.write.mode('overwrite').partitionBy(\"Store_ID\").format(\"parquet\").save(\"/Users/sandip/Data/spark/SCD2/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Day 1 Load \n",
    "day1=spark.read.csv(\"/Users/sandip/Data/spark/SCD2/Day1.csv\",header='true',inferSchema='true')\n",
    "\n",
    "#Rename columns of dataframe\n",
    "day1=day1.withColumnRenamed('Store_ID','delta_Store_ID').withColumnRenamed('Location','delta_Location').withColumnRenamed('Value','delta_Value')\n",
    "\n",
    "#read existing data \n",
    "history= spark.read.parquet(\"/Users/sandip/Data/spark/SCD2/output/\")\n",
    "#Get maximum number from sk column to increment for day 1 load\n",
    "max_account_sk_id_rdd_list = history.agg({'store_id_sk': 'max'}).rdd.map(list)\n",
    "for line in max_account_sk_id_rdd_list.collect():\n",
    "    max_store_id_sk=line[0]\n",
    "#Capture modified records from day1 load\n",
    "cdc_change_df = day1.join(history ,(history.Store_ID == day1.delta_Store_ID) ,'left_outer' ).where((history['Value'] != day1['delta_Value']) & (history['current_flag'] == lit(eff_flag_curr)))\n",
    "#Capture new records from day1 load\n",
    "cdc_new_df = day1.join(history ,(day1.delta_Store_ID == history.Store_ID) ,'left_outer' ).where(history['Value'].isNull())\n",
    "#Mearge both\n",
    "cdc_all_record_df = cdc_new_df.unionAll(cdc_change_df)\n",
    "#Add sk cloumn and other\n",
    "delta_sk_lkp_df = cdc_all_record_df.select('delta_Store_ID').withColumn('delta_Store_sk_ID', F.row_number().over(Window.partitionBy(lit(1)).orderBy(lit(1))))\n",
    "cdc_all_record_sk_df = cdc_all_record_df.join(broadcast(delta_sk_lkp_df) ,'delta_Store_ID', 'inner').withColumn('max_store_sk_id',lit(max_store_id_sk)).withColumn('delta_Store_sk_ID',lit(max_store_id_sk + delta_sk_lkp_df.delta_Store_sk_ID)).withColumn('delta_eff_start_date',lit(current_bus_dt)).withColumn('delta_eff_end_date',lit(eff_close_dt)).withColumn('delta_load_tm',lit(load_tm)).withColumn('delta_current_flag',lit(eff_flag_curr)).withColumn('is_delta',lit('Y'))\n",
    "\n",
    "#Adding temp flag to capture delta\n",
    "cdc_all_record_sk_ld_df = cdc_all_record_sk_df.select('delta_Location','delta_Value','delta_Store_sk_ID','delta_eff_start_date','delta_eff_end_date','delta_load_tm','delta_current_flag','is_delta','delta_Store_ID')\n",
    "cdc_change_df_id=cdc_change_df.select(\"Location\",\"Value\",\"store_id_sk\",\"eff_start_date\",\"eff_end_date\",\"load_tm\",\"current_flag\",\"Store_ID\").withColumn('is_delta',lit('N'))\n",
    "cdc_change_df_id=cdc_change_df_id.select(\"Location\",\"Value\",\"store_id_sk\",\"eff_start_date\",\"eff_end_date\",\"load_tm\",\"current_flag\",'is_delta',\"Store_ID\")\n",
    "hist_cdc_change_df = cdc_change_df_id.withColumn('eff_end_date',lit(load_tm)).withColumn('current_flag',lit(eff_flag_non_curr))\n",
    "\n",
    "#mearge final dataframe\n",
    "final=hist_cdc_change_df.unionAll(cdc_all_record_sk_ld_df)\n",
    "\n",
    "#final.write.mode('append').partitionBy(\"Store_ID\").format(\"parquet\").save(\"/Users/sandip/Data/spark/SCD2/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Push new records \n",
    "x_final=final.where(final['current_flag'] == lit(eff_flag_curr))\n",
    "x_final.write.mode('append').partitionBy(\"Store_ID\").format(\"parquet\").save(\"/Users/sandip/Data/spark/SCD2/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Modify existing records for maintaing SCD2\n",
    "store_list = hist_cdc_change_df.select('Store_ID').rdd.map(list)\n",
    "y=store_list.collect()\n",
    "for x in range(len(y)):\n",
    "    history= spark.read.parquet(\"/Users/sandip/Data/spark/SCD2/output/\")\n",
    "    updated_df = history.where((history[\"Store_ID\"]==lit(y[x][0])) & (history['current_flag'] == lit(eff_flag_non_curr))& (history['is_delta'] == lit(\"N\")))\n",
    "    updated_new_df = history.where((history[\"Store_ID\"]==lit(y[x][0])) & (history['current_flag'] == lit(eff_flag_curr))& (history['is_delta'] == lit(\"Y\")))\n",
    "    updated_df.count()\n",
    "    new_df=hist_cdc_change_df.where(hist_cdc_change_df[\"Store_ID\"]==lit(y[x][0]))\n",
    "    new_df.count()\n",
    "    final_df=updated_df.unionAll(updated_new_df).unionAll(new_df)\n",
    "    final_df.write.mode('overwrite').format(\"parquet\").save(\"/Users/sandip/Data/spark/SCD2/Temp/\")\n",
    "    x_df=spark.read.parquet(\"/Users/sandip/Data/spark/SCD2/Temp/\")\n",
    "    x_df.write.mode('overwrite').format(\"parquet\").save(\"/Users/sandip/Data/spark/SCD2/output/Store_ID=\"+y[x][0])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
